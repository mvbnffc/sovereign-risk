{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3117645-5f34-4d37-900e-45ed850d544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import country_converter as coco\n",
    "from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from sovereign.flood import build_basin_curves, BasinLossCurve, risk_data_future_shift, run_simulation, extract_sectoral_losses\n",
    "from sovereign.macroeconomic import run_flood_sim_for_macro\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b825e8e-2e40-4bb1-ab6c-bd6e7b9d32ad",
   "metadata": {},
   "source": [
    "### User Config and Inputs for ENTIRE run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec644c4-162e-48a1-9373-08e71a6b62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent # find project root\n",
    "risk_basin_path = os.path.join(root, 'outputs', 'flood', 'risk', 'basins', 'risk_basins.csv')\n",
    "copula_path = os.path.join(root, 'outputs', 'flood', 'dependence', 'copulas')\n",
    "risk_data = pd.read_csv(risk_basin_path)\n",
    "future_rp_shifts = pd.read_csv(os.path.join(root, 'outputs', 'flood', 'future', 'basin_rp_shifts.csv'))\n",
    "copula_random_numbers = pd.read_parquet(os.path.join(copula_path, \"copula_random_numbers.gzip\"))\n",
    "macro_presim = pd.read_csv(os.path.join(root, 'outputs', 'macro', 'DIGNAD_presim_n1000_noadapt.csv'))\n",
    "economic = pd.read_csv(os.path.join(root, 'inputs', 'credit_risk', 'economic.csv')) # df1\n",
    "SP_data = pd.read_csv(os.path.join(root, 'inputs', 'credit_risk', 'T3.csv'))\n",
    "PD_data = pd.read_csv(os.path.join(root, 'inputs', 'credit_risk', 'PD_ratings.csv'), header=None)\n",
    "# Drop first \"unnamed column\"\n",
    "risk_data = risk_data.iloc[:, 1:]\n",
    "# Add AEP column\n",
    "risk_data['AEP'] = 1 / risk_data['RP']\n",
    "# Add a column converting current prorection level into AEP\n",
    "risk_data['Pr_L_AEP'] = np.where(risk_data['Pr_L'] == 0, 0, 1 / risk_data['Pr_L']) # using numpy where avoids zero division errors\n",
    "risk_data.reset_index(drop=True, inplace=True)\n",
    "adaptation_aep = 0.01 # 100-year flood protection\n",
    "n_years = 10000 # number of years to simulate\n",
    "Thai_GDP = 496e9 # 2022 numbers in USD\n",
    "# future_hydro = 'jules-w2'\n",
    "# future_epoch = 2070\n",
    "# future_scenario = 'ssp585'\n",
    "# future_stat = 'q90'\n",
    "# National GVA figures from DOSE\n",
    "agr_GVA = 42880325598\n",
    "man_GVA = 162659433017\n",
    "ser_GVA = 316647741231\n",
    "# Disaggregate output losses\n",
    "TRADABLE_SHARES = {\n",
    "    \"Agriculture\": 1.0,\n",
    "    \"Manufacturing\": 0.7,\n",
    "    \"Service\": 0.5,\n",
    "}\n",
    "EPOCHS = ['2030', '2040', '2050', '2060', '2070'] # what future epochs are we interested in?\n",
    "SCENARIOS = ['ssp126', 'ssp370', 'ssp585'] # what climate scenarios are we intersted in?\n",
    "STATS = ['q05', 'q50', 'q95', 'mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c118c9-c79f-4a3f-b973-f816c2b31b98",
   "metadata": {},
   "source": [
    "#### Prepare credit risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8c48f9-8ce6-4c4d-98ad-e619c11b44c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Congo D.R. not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Ras Al KhaImah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n",
      "Sharjah not found in regex\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=2000, oob_score=True, random_state=77)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=2000, oob_score=True, random_state=77)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=2000, oob_score=True, random_state=77)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_losses = SP_data[\"GDP_per_capita\"] / 100\n",
    "\n",
    "def polyfit_raw(x, y):\n",
    "    coeffs = np.polyfit(x, y, 3)\n",
    "    return coeffs[::-1]   # reverse to match β0 + β1x + β2x² + β3x³\n",
    "\n",
    "# NGGD\n",
    "NGGD = np.log(SP_data[\"NGGD\"])\n",
    "b_NGGD = polyfit_raw(gdp_losses, NGGD)\n",
    "\n",
    "# GGB (filter < 0)\n",
    "sub = SP_data[SP_data[\"GGB\"] < 0]\n",
    "GGB = np.log(-sub[\"GGB\"])\n",
    "b_GGB = polyfit_raw(sub[\"GDP_per_capita\"]/100, GGB)\n",
    "\n",
    "# NNED (filter > 0)\n",
    "sub = SP_data[SP_data[\"NNED\"] > 0]\n",
    "NNED = np.log(sub[\"NNED\"])\n",
    "b_NNED = polyfit_raw(sub[\"GDP_per_capita\"]/100, NNED)\n",
    "\n",
    "# CAB\n",
    "CAB = np.log(-SP_data[\"CAB\"])\n",
    "b_CAB = polyfit_raw(gdp_losses, CAB)\n",
    "\n",
    "# =========================================================\n",
    "# Helper: apply polynomial\n",
    "# =========================================================\n",
    "\n",
    "def equa(A, b0, b1, b2, b3):\n",
    "    return b0 + b1*A + b2*A**2 + b3*A**3\n",
    "\n",
    "# Real GDP growth (pct change)\n",
    "economic[\"S_RealGDPgrowth\"] = economic.groupby(\"CountryName\")[\"S_GDPpercapitaUS\"].pct_change()\n",
    "\n",
    "economic = economic[\n",
    "    [\n",
    "        \"CountryName\",\"Year\",\"scale20\",\"S_GDPpercapitaUS\",\n",
    "        \"S_RealGDPgrowth\",\"S_NetGGdebtGDP\",\"S_GGbalanceGDP\",\n",
    "        \"S_NarrownetextdebtCARs\",\"S_CurrentaccountbalanceGDP\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "cc = coco.CountryConverter()\n",
    "economic[\"ISO2\"] = cc.convert(economic[\"CountryName\"], to=\"ISO3\")\n",
    "\n",
    "# Build baseline data\n",
    "Baseline = economic.copy()\n",
    "Baseline[\"ln_S_GDPpercapitaUS\"] = np.log(Baseline[\"S_GDPpercapitaUS\"])\n",
    "\n",
    "Baseline = Baseline[Baseline[\"Year\"] > 2014]\n",
    "Baseline = Baseline.dropna()\n",
    "\n",
    "rating = PD_data.iloc[:, 0]\n",
    "default = PD_data.iloc[:, 1]\n",
    "b_PD = polyfit_raw(rating, default)\n",
    "\n",
    "def implement_PD_equation(r):\n",
    "    PD = equa(r, *b_PD)\n",
    "    return np.clip(PD, 0, 100)\n",
    "\n",
    "def calculate_spreads(C_0, notches):\n",
    "    C_0 = np.asarray(C_0)\n",
    "    notches = np.asarray(notches)\n",
    "    return (-282.51 * notches) + (14.23 * notches * C_0)\n",
    "\n",
    "def estimate_country_scenario(country_, loss_):\n",
    "\n",
    "    # extract baseline for 2020\n",
    "    gdp_per_capita = Baseline[(Baseline[\"CountryName\"] == country_) & (Baseline[\"Year\"] == 2020)].iloc[0]\n",
    "\n",
    "    gdp_pc_value = gdp_per_capita[\"S_GDPpercapitaUS\"]\n",
    "\n",
    "    estimation = pd.DataFrame({\n",
    "        \"CountryName\": [country_],\n",
    "        \"loss\": [loss_]\n",
    "    })\n",
    "\n",
    "    estimation[\"ISO2\"] = cc.convert(estimation[\"CountryName\"], to=\"ISO3\")\n",
    "    estimation[\"ln_S_GDPpercapitaUS\"] = np.log(gdp_pc_value * (1 - loss_))\n",
    "    estimation[\"S_RealGDPgrowth\"] = -loss_\n",
    "\n",
    "    # baseline values\n",
    "    for col in [\"S_NetGGdebtGDP\",\"S_GGbalanceGDP\",\"S_NarrownetextdebtCARs\",\"S_CurrentaccountbalanceGDP\"]:\n",
    "        estimation[col] = gdp_per_capita[col]\n",
    "\n",
    "    A = -loss_\n",
    "\n",
    "    # apply fitted polynomial adjustments\n",
    "    estimation[\"S_NetGGdebtGDP\"] += np.exp(equa(A, *b_NGGD))\n",
    "    estimation[\"S_GGbalanceGDP\"] += -np.exp(equa(A, *b_GGB))\n",
    "    estimation[\"S_NarrownetextdebtCARs\"] += np.exp(equa(A, *b_NNED))\n",
    "    estimation[\"S_CurrentaccountbalanceGDP\"] += -np.exp(equa(A, *b_CAB))\n",
    "\n",
    "    return estimation\n",
    "\n",
    "features = [\n",
    "    \"ln_S_GDPpercapitaUS\",\n",
    "    \"S_RealGDPgrowth\",\n",
    "    \"S_NetGGdebtGDP\",\n",
    "    \"S_GGbalanceGDP\",\n",
    "    \"S_NarrownetextdebtCARs\",\n",
    "    \"S_CurrentaccountbalanceGDP\"\n",
    "]\n",
    "\n",
    "X_train = Baseline[features]\n",
    "y_train = Baseline[\"scale20\"]\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=2000,\n",
    "    random_state=77,\n",
    "    oob_score=True\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f38e77-9194-4d42-8076-04cff4ac3513",
   "metadata": {},
   "source": [
    "#### Prepare basin curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2c1a47-d2a5-4d5a-bf29-01961bb8b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to loop over\n",
    "future_hydro = 'jules-w2'\n",
    "SCENARIOS = [\"ssp126\", \"ssp370\", \"ssp585\"]\n",
    "EPOCHS    = [2030, 2040, 2050, 2060, 2070]   # or whatever you use\n",
    "STATS     = [\"q05\", \"q50\", \"q95\", \"mean\"]\n",
    "# Adjust baseline risk to future risk \n",
    "future_risk = {}  # (scenario, epoch, stat) -> adjusted risk_data\n",
    "for scenario, epoch, stat in product(SCENARIOS, EPOCHS, STATS):\n",
    "    future_risk[(scenario, epoch, stat)] = risk_data_future_shift(\n",
    "        risk_data,\n",
    "        future_rp_shifts,\n",
    "        future_hydro,\n",
    "        scenario,\n",
    "        epoch,\n",
    "        stat,\n",
    "        degrade_protection=True\n",
    "    )\n",
    "# Build baseline curves\n",
    "baseline_curves: dict[int, BasinLossCurve] = build_basin_curves(risk_data)\n",
    "future_curves = {}\n",
    "# Build future curves\n",
    "for key, future_risk_data in future_risk.items():\n",
    "    future_curves[key] = build_basin_curves(future_risk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d595a03-7f5f-4829-8f84-92c250e027f4",
   "metadata": {},
   "source": [
    "#### Run full flood simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc991f3-4e2f-4774-b883-c9edc6a7b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIGNAD INTERPOLATOR\n",
    "# Create interpolators\n",
    "X = macro_presim[['dY_T', 'dY_N', 'dK_priv', 'dK_pub']].values\n",
    "y = macro_presim['gdp_avg'].values\n",
    "\n",
    "linear_interp = LinearNDInterpolator(X, y)\n",
    "nearest_interp = NearestNDInterpolator(X, y)\n",
    "\n",
    "# Interpolation function for monte carlo analysis\n",
    "def interpolate_gdp(params):\n",
    "    gdp = linear_interp(params)\n",
    "    if np.isnan(gdp): # if outside convex hull\n",
    "        gdp = nearest_interp(params)\n",
    "    if np.all(params == 0): # no disaster\n",
    "        gdp = 0.0\n",
    "    return gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af631fb8-fa88-4099-ab95-edc685cbdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def flood_aggregates_one_year(basin_curves, random_ns_row):\n",
    "    \"\"\"\n",
    "    Returns annual monetary losses aggregated across basins for the 5 sectors needed downstream.\n",
    "    \"\"\"\n",
    "    ag = man = serv = priv = pub = 0.0\n",
    "\n",
    "    for basin_id, curve in basin_curves.items():\n",
    "        basin_str = str(int(basin_id))\n",
    "        if basin_str not in random_ns_row:\n",
    "            continue\n",
    "\n",
    "        aep_event = 1 - random_ns_row[basin_str]\n",
    "\n",
    "        ag   += curve.loss_at_event_aep(aep_event, scenario=\"baseline\", sector=\"Agriculture\")\n",
    "        man  += curve.loss_at_event_aep(aep_event, scenario=\"baseline\", sector=\"Manufacturing\")\n",
    "        serv += curve.loss_at_event_aep(aep_event, scenario=\"baseline\", sector=\"Service\")\n",
    "        priv += curve.loss_at_event_aep(aep_event, scenario=\"baseline\", sector=\"Private\")\n",
    "        pub  += curve.loss_at_event_aep(aep_event, scenario=\"baseline\", sector=\"Public\")\n",
    "\n",
    "    return ag, man, serv, priv, pub\n",
    "\n",
    "def run_integrated_yearwise(\n",
    "    country_: str,\n",
    "    baseline_curves: dict,\n",
    "    future_curves: dict,              # (scenario, epoch, stat) -> basin_curves\n",
    "    copula_random_ns: pd.DataFrame,\n",
    "    n_years: int,\n",
    "    # mapping constants\n",
    "    agr_GVA: float,\n",
    "    man_GVA: float,\n",
    "    ser_GVA: float,\n",
    "    tradable_shares: dict,\n",
    "    thai_gdp: float,\n",
    "):\n",
    "    # Combine curve sets: baseline + futures\n",
    "    curve_sets = {\"baseline\": baseline_curves, **future_curves}\n",
    "\n",
    "    # Precompute denominators for macro mapping\n",
    "    tradable_output_baseline = (\n",
    "        agr_GVA * tradable_shares[\"Agriculture\"] +\n",
    "        man_GVA * tradable_shares[\"Manufacturing\"] +\n",
    "        ser_GVA * tradable_shares[\"Service\"]\n",
    "    )\n",
    "    nontrad_output_baseline = (\n",
    "        agr_GVA * (1 - tradable_shares[\"Agriculture\"]) +\n",
    "        man_GVA * (1 - tradable_shares[\"Manufacturing\"]) +\n",
    "        ser_GVA * (1 - tradable_shares[\"Service\"])\n",
    "    )\n",
    "\n",
    "    # Pull baseline rating once (like your function does)\n",
    "    original_rating = float(\n",
    "        Baseline.loc[\n",
    "            (Baseline[\"CountryName\"] == country_) &\n",
    "            (Baseline[\"Year\"] == 2020),\n",
    "            \"scale20\"\n",
    "        ].iloc[0]\n",
    "    )\n",
    "    original_pd = implement_PD_equation(original_rating)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for t in tqdm(range(n_years), desc=\"Simulating years\"):\n",
    "        random_ns_row = copula_random_ns.loc[t]\n",
    "\n",
    "        for key, basin_curves in curve_sets.items():\n",
    "            # --- 1) FLOOD ---\n",
    "            ag, man, serv, priv, pub = flood_aggregates_one_year(basin_curves, random_ns_row)\n",
    "\n",
    "            # --- 2) MAP TO MACRO PARAMS (what interpolate_gdp expects) ---\n",
    "            # shocks as *fractions* (matching your DIGNAD-ish mapping)\n",
    "            trad_out = (\n",
    "                ag  * tradable_shares[\"Agriculture\"] +\n",
    "                man * tradable_shares[\"Manufacturing\"] +\n",
    "                serv* tradable_shares[\"Service\"]\n",
    "            )\n",
    "            nontrad_out = (\n",
    "                ag  * (1 - tradable_shares[\"Agriculture\"]) +\n",
    "                man * (1 - tradable_shares[\"Manufacturing\"]) +\n",
    "                serv* (1 - tradable_shares[\"Service\"])\n",
    "            )\n",
    "\n",
    "            dY_T   = trad_out / tradable_output_baseline\n",
    "            dY_N   = nontrad_out / nontrad_output_baseline\n",
    "            dK_priv= priv / thai_gdp\n",
    "            dK_pub = pub  / thai_gdp\n",
    "\n",
    "            # Build params vector in the order your interpolator expects.\n",
    "            # You’ll need to align this with how linear_interp() was trained.\n",
    "            params = np.array([dY_T, dY_N, dK_priv, dK_pub], dtype=float)\n",
    "\n",
    "            # --- 3) MACRO (GDP loss) ---\n",
    "            gdp_loss = float(interpolate_gdp(params))\n",
    "\n",
    "            # --- 4) CREDIT ---\n",
    "            # Your estimate_country_scenario expects a \"loss_\" scalar. Here we pass gdp_loss.\n",
    "            estimation = estimate_country_scenario(country_, (gdp_loss * -1 / 100))\n",
    "            predicted_rating = float(rf.predict(estimation[features])[0])\n",
    "            predicted_pd = float(implement_PD_equation(predicted_rating))\n",
    "            spread_delta = float(calculate_spreads(original_rating, (predicted_rating - original_rating)))\n",
    "\n",
    "            # --- store ---\n",
    "            if key == \"baseline\":\n",
    "                scenario = \"baseline\"\n",
    "                epoch = stat = None\n",
    "            else:\n",
    "                scenario, epoch, stat = key  # key is (ssp, epoch, stat)\n",
    "\n",
    "            rows.append({\n",
    "                \"year_index\": t,\n",
    "                \"scenario\": scenario,\n",
    "                \"epoch\": epoch,\n",
    "                \"stat\": stat,\n",
    "                # flood outputs (optional, but useful)\n",
    "                \"AGR_loss\": ag,\n",
    "                \"MAN_loss\": man,\n",
    "                \"SER_loss\": serv,\n",
    "                \"PRI_dam\": priv,\n",
    "                \"PUB_dam\": pub,\n",
    "                # macro shocks\n",
    "                \"dY_T\": dY_T,\n",
    "                \"dY_N\": dY_N,\n",
    "                \"dK_priv\": dK_priv,\n",
    "                \"dK_pub\": dK_pub,\n",
    "                \"gdp_loss\": gdp_loss,\n",
    "                # credit outputs\n",
    "                \"original_rating\": original_rating,\n",
    "                \"predicted_rating\": predicted_rating,\n",
    "                \"original_pd\": original_pd,\n",
    "                \"predicted_pd\": predicted_pd,\n",
    "                \"spread_delta\": spread_delta,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469f32bf-be85-4969-ad7a-9434e08a8e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating years: 100%|██████████████████████████████████████████████████████████████| 100/100 [15:27<00:00,  9.27s/it]\n"
     ]
    }
   ],
   "source": [
    "full_sim = run_integrated_yearwise(\n",
    "    country_=\"Thailand\",\n",
    "    baseline_curves=baseline_curves,\n",
    "    future_curves=future_curves,\n",
    "    copula_random_ns=copula_random_numbers,\n",
    "    n_years=100,\n",
    "    agr_GVA=agr_GVA,\n",
    "    man_GVA=man_GVA,\n",
    "    ser_GVA=ser_GVA,\n",
    "    tradable_shares=TRADABLE_SHARES,\n",
    "    thai_gdp=Thai_GDP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c03e13-413d-4d72-bfb2-072a53783f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sim.to_csv('test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ad72c1a-0678-43f8-b2cc-0870a245cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sim.to_csv('full_sim_n1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7caf0299-32dd-4f72-a682-9eb5259ceccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.396\n"
     ]
    }
   ],
   "source": [
    "gdp_loss = -1.0\n",
    "estimation = estimate_country_scenario('Thailand',gdp_loss)\n",
    "predicted_rating = float(rf.predict(estimation[features])[0])\n",
    "print(predicted_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff6650-52a1-433b-8293-3dabc42dbd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sovereign-risk",
   "language": "python",
   "name": "sovereign-risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
