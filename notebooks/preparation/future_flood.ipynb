{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277e9490-f7d5-407a-950c-baa100b98c62",
   "metadata": {},
   "source": [
    "### Preparation Notebook - Future Flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51375082-6209-41d9-b8e4-507b9fabaad8",
   "metadata": {},
   "source": [
    "\n",
    "This notebook processes ISIMIP future forced hyrdological model outputs to calculate basin level future flood-frequency changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9be8122-5d32-4435-ac75-e716ccb919ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import live code changes in\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "from sovereign.utils import pot_with_optimal_threshold, df_to_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30dea8-0849-4230-b6f7-5c94976513ed",
   "metadata": {},
   "source": [
    "##### 1. User Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e238e57-9c94-4e23-95bc-01ad57942861",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_periods = [10, 20, 50, 75, 100, 200, 500] # return periods of interest \n",
    "# Sampling periods for EVA\n",
    "sample_periods = {\n",
    "    \"historical\": (\"1965-01-01\", \"2014-12-31\"),\n",
    "    \"2030\": (\"2005-01-01\", \"2054-12-31\"),\n",
    "    \"2040\": (\"2015-01-01\", \"2064-12-31\"),\n",
    "    \"2050\": (\"2025-01-01\", \"2074-12-31\"),\n",
    "    \"2060\": (\"2035-01-01\", \"2084-12-31\"),\n",
    "    \"2070\": (\"2045-01-01\", \"2094-12-31\")\n",
    "}\n",
    "# For rasterization\n",
    "EPOCHS = ['2030', '2040', '2050', '2060', '2070'] # what future epochs are we interested in?\n",
    "SCENARIOS = ['ssp126', 'ssp370', 'ssp585'] # what climate scenarios are we intersted in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f25228-0335-4d4f-a800-5645cb24f01c",
   "metadata": {},
   "source": [
    "##### 2. Set filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d78df0-3a81-4c43-be62-96dbd1404bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent.parent # find project root\n",
    "isimip_path = Path(os.path.join(root, 'inputs', 'flood', 'future', 'isimip', 'THA')) # this directory contains relevant ISIMIP layers\n",
    "prelim_output_path = Path(os.path.join(root, 'outputs', 'flood', 'future', 'prelim')) \n",
    "basin_path = Path(os.path.join(root, 'inputs', 'boundaries', 'basins', 'BA_THA_lev06.shp'))\n",
    "RASTER_DIR = Path(os.path.join(root, 'outputs', 'flood', 'future', 'maps')) # hwere to save the rasters\n",
    "FUTURE_BASINS = Path(os.path.join(root, 'outputs', 'flood', 'future', 'basin_rp_shifts.csv')) # where to save the basin RP shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5cfbf-8962-412f-b055-7de8bffe0d55",
   "metadata": {},
   "source": [
    "##### 3. Run POT EVA on ISIMIP layers (Parallelized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57dd255-a98e-4386-85a5-5005af3c46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model THA_dis_jules-w2_gfdl-esm4_ssp126.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_gfdl-esm4_ssp370.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_gfdl-esm4_ssp585.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ipsl-cm6a-lr_ssp126.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ipsl-cm6a-lr_ssp370.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ipsl-cm6a-lr_ssp585.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mpi-esm1-2-hr_ssp126.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mpi-esm1-2-hr_ssp370.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mpi-esm1-2-hr_ssp585.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mri-esm2-0_ssp126.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mri-esm2-0_ssp370.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_mri-esm2-0_ssp585.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ukesm1-0-ll_ssp126.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ukesm1-0-ll_ssp370.nc, results already exist.\n",
      "Skipping model THA_dis_jules-w2_ukesm1-0-ll_ssp585.nc, results already exist.\n"
     ]
    }
   ],
   "source": [
    "def process_cell(lat, lon, dis_data, sample_periods, return_periods, model, climate_scenario):\n",
    "    \"\"\"Process a single grid cell across all periods and return periods.\"\"\"\n",
    "    cell_results = []\n",
    "    for period_name, (start_date, end_date) in sample_periods.items():\n",
    "        period_data = dis_data.sel(time=slice(start_date, end_date)).values\n",
    "        if np.std(period_data) == 0.0:\n",
    "            continue\n",
    "        for return_period in return_periods:\n",
    "            try:\n",
    "                best, diagnostics = pot_with_optimal_threshold(\n",
    "                    x=period_data,\n",
    "                    candidate_ps=[0.90, 0.92, 0.94, 0.96, 0.97, 0.98, 0.99],\n",
    "                    return_period=return_period\n",
    "                )\n",
    "                if best is None:\n",
    "                    continue\n",
    "                cell_results.append({\n",
    "                    \"latitude\": lat,\n",
    "                    \"longitude\": lon,\n",
    "                    \"return_level\": best['qT'],\n",
    "                    \"log_return_level\": best['log_qT'],\n",
    "                    \"std_log_return_level\": best['var_log_qT'] ** 0.5,\n",
    "                    \"q5\": np.exp(best['log_qT'] - 1.96 * best['var_log_qT'] ** 0.5),\n",
    "                    \"q95\": np.exp(best['log_qT'] + 1.96 * best['var_log_qT'] ** 0.5),\n",
    "                    \"xi_hat\": best['xi'],\n",
    "                    \"sigma_hat\": best['sigma'],\n",
    "                    \"selected_threshold\": best['p'],\n",
    "                    \"threshold_discharge\": best['u'],\n",
    "                    \"num_exceedances\": best['n_exc'],\n",
    "                    \"ks_pvalue\": best['ks_pvalue'],\n",
    "                    \"return_period\": return_period,\n",
    "                    \"model\": model,\n",
    "                    \"period\": period_name,\n",
    "                    \"period_start_date\": start_date,\n",
    "                    \"period_end_date\": end_date,\n",
    "                    \"climate_scenario\": climate_scenario,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error at lat {lat}, lon {lon}: {e}\")\n",
    "    return cell_results\n",
    "\n",
    "# Initialize results dataframe\n",
    "pot_results = pd.DataFrame()\n",
    "models = [f for f in os.listdir(isimip_path) if f.endswith('.nc')]\n",
    "\n",
    "for model in models:\n",
    "    parquet_path = os.path.join(prelim_output_path, f'{model}.parquet')\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"Skipping model {model}, results already exist.\")\n",
    "        pot_results = pd.concat([pot_results, pd.read_parquet(parquet_path)], ignore_index=True)\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing model: {model}\")\n",
    "    dataset = xr.open_dataset(os.path.join(isimip_path, model))\n",
    "    latitudes = dataset['lat'].values\n",
    "    longitudes = dataset['lon'].values\n",
    "    climate_scenario = model.split('_')[-1].replace('.nc', '')\n",
    "\n",
    "    dis = dataset['dis']\n",
    "\n",
    "    # Run cell-level processing in parallel\n",
    "    results_nested = Parallel(n_jobs=-1, backend='loky')(\n",
    "        delayed(process_cell)(\n",
    "            lat, lon,\n",
    "            dis.sel(lat=lat, lon=lon, method=\"nearest\"),\n",
    "            sample_periods, return_periods, model, climate_scenario\n",
    "        )\n",
    "        for lat, lon in[(lat, lon) for lat in latitudes for lon in longitudes]\n",
    "    )\n",
    "\n",
    "    # Flatten list of lists\n",
    "    flat_results = [row for cell in results_nested for row in cell]\n",
    "    model_results = pd.DataFrame(flat_results)\n",
    "\n",
    "    Path(prelim_output_path).mkdir(parents=True, exist_ok=True)\n",
    "    model_results.to_parquet(parquet_path)\n",
    "    pot_results = pd.concat([pot_results, model_results], ignore_index=True)\n",
    "\n",
    "pot_results = pot_results.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a4972-c31e-4bab-9e8d-be3867a77d7a",
   "metadata": {},
   "source": [
    "##### 5. Combine results to clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e3d354-c635-4312-97c5-5a0fc4f6e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_levels = pot_results[pot_results['period'] == 'historical'][[\n",
    "    'latitude', 'longitude', 'climate_scenario', 'model', 'return_period', 'return_level', 'q5', 'q95', 'xi_hat', 'sigma_hat', 'selected_threshold']\n",
    "    ]\n",
    "future_with_baseline =pd.merge(\n",
    "    pot_results,\n",
    "    baseline_levels,\n",
    "    on=['latitude', 'longitude', 'climate_scenario', 'model', 'return_period'],\n",
    "    suffixes=('', '_baseline')\n",
    ")\n",
    "# Calculate multipliers\n",
    "future_with_baseline['multiplier'] = future_with_baseline['return_level'] / future_with_baseline['return_level_baseline']\n",
    "future_with_baseline['multiplier_q5'] = future_with_baseline['q5'] / future_with_baseline['return_level_baseline']\n",
    "future_with_baseline['multiplier_q95'] = future_with_baseline['q95'] / future_with_baseline['return_level_baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bd42e-8431-43bc-8cf7-a0590f0595d5",
   "metadata": {},
   "source": [
    "##### 6. Calculate adjusted return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a155cd7c-cc92-4748-af22-5bfdcc42ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjusted_return_period(\n",
    "    xi_hat,\n",
    "    sigma_hat,\n",
    "    threshold_discharge,  \n",
    "    return_level_scenario,\n",
    "    return_level_baseline,\n",
    "    baseline_return_period\n",
    "):\n",
    "    # Basic checks\n",
    "    vals = [xi_hat, sigma_hat, threshold_discharge, return_level_scenario, return_level_baseline, baseline_return_period]\n",
    "    if any(v is None or np.isnan(v) for v in vals):\n",
    "        return np.nan\n",
    "    if sigma_hat <= 0 or baseline_return_period <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    if abs(xi_hat) < 1e-6:\n",
    "        return float(baseline_return_period * np.exp((return_level_baseline - return_level_scenario) / sigma_hat))\n",
    "\n",
    "    # ξ ≠ 0: general GP case\n",
    "    num = 1 + xi_hat * (return_level_baseline - threshold_discharge) / sigma_hat\n",
    "    den = 1 + xi_hat * (return_level_scenario - threshold_discharge) / sigma_hat\n",
    "\n",
    "    # Domain check for the GP (must be > 0)\n",
    "    if num <= 0 or den <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    ratio = (num / den) ** (1.0 / xi_hat)\n",
    "    return float(baseline_return_period * ratio)\n",
    "\n",
    "future_with_baseline['adjusted_return_period'] = future_with_baseline.apply(\n",
    "    lambda row: get_adjusted_return_period(\n",
    "        xi_hat=row['xi_hat'],\n",
    "        sigma_hat=row['sigma_hat'],\n",
    "        threshold_discharge=row['selected_threshold'],\n",
    "        return_level_scenario=row['return_level'],\n",
    "        return_level_baseline=row['return_level_baseline'],\n",
    "        baseline_return_period=row['return_period']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "future_with_baseline['adjusted_return_period_q5'] = future_with_baseline.apply(\n",
    "    lambda row: get_adjusted_return_period(\n",
    "        xi_hat=row['xi_hat'],\n",
    "        sigma_hat=row['sigma_hat'],\n",
    "        threshold_discharge=row['selected_threshold'],\n",
    "        return_level_scenario=row['q5'],\n",
    "        return_level_baseline=row['return_level_baseline'],\n",
    "        baseline_return_period=row['return_period']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "future_with_baseline['adjusted_return_period_q95'] = future_with_baseline.apply(\n",
    "    lambda row: get_adjusted_return_period(\n",
    "        xi_hat=row['xi_hat'],\n",
    "        sigma_hat=row['sigma_hat'],\n",
    "        threshold_discharge=row['selected_threshold'],\n",
    "        return_level_scenario=row['q95'],\n",
    "        return_level_baseline=row['return_level_baseline'],\n",
    "        baseline_return_period=row['return_period']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "# Clean the dataframe for next step (extract climate model and hydro model names and add to column)\n",
    "future_with_baseline['noext'] = future_with_baseline['model'].str.replace('.nc', '', regex=False)\n",
    "parts = future_with_baseline['noext'].str.split('_', expand=True) # split by '_'\n",
    "future_with_baseline['hydro'] = parts[2] # add a specific hydro model column\n",
    "future_with_baseline['climate'] = parts[3] # add a specific climate model column\n",
    "future_with_baseline = future_with_baseline.drop(columns=['noext']) # clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b333a-20f3-4ac0-93cd-f553b2f04364",
   "metadata": {},
   "source": [
    "##### 7. Create ISIMIP future RP change rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae233df6-ec8c-489a-8358-94b3b0accf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rasterizing: 100%|█████████████████████████████████████████████████████████████████| 525/525 [00:47<00:00, 11.09file/s]\n"
     ]
    }
   ],
   "source": [
    "# Find all unique hydro model and climate model pairs in the dataframe\n",
    "model_pairs = (\n",
    "    future_with_baseline[['hydro', 'climate']]\n",
    "    .drop_duplicates()\n",
    "    .itertuples(index=False, name=None)\n",
    ")\n",
    "model_pairs = list(model_pairs) # Make it a list\n",
    "\n",
    "# Create raster directory if it doesn't already exoist\n",
    "RASTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build full combination list\n",
    "all_jobs = list(product(model_pairs, SCENARIOS, EPOCHS, return_periods))\n",
    "\n",
    "for (hydro, clim), scen, epoch, rp in tqdm(all_jobs, desc=\"Rasterizing\", unit=\"file\"):\n",
    "    # Set output filepath\n",
    "    out_name = f\"{hydro}_{clim}_{scen}_{epoch}_rp{rp:03d}.tif\"\n",
    "    out_path = Path(os.path.join(RASTER_DIR, out_name))\n",
    "    # Skip if file already exists\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "        \n",
    "    # Extract dataframe subset\n",
    "    sub = future_with_baseline[\n",
    "        (future_with_baseline['hydro'] == hydro) &\n",
    "        (future_with_baseline['climate'] == clim) & \n",
    "        (future_with_baseline['climate_scenario'] == scen) & \n",
    "        (future_with_baseline['period'] == epoch) & \n",
    "        (future_with_baseline['return_period'] == rp)\n",
    "    ]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "        \n",
    "    # Convert to raster\n",
    "    df_to_raster(sub, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf763c11-8a2f-407b-b0ad-65d534d9e9f4",
   "metadata": {},
   "source": [
    "##### 8. Calculate hydrological basin-level RP change statistics (running zonal stats on ISIMIP rasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be656c56-434b-432e-920d-f66f07ebe7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Zonal Stats: 100%|███████████████████████████████████████████████████████████| 525/525 [18:17<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the basins file\n",
    "basins = gpd.read_file(basin_path)\n",
    "basins = basins[['HYBAS_ID', 'geometry']] # just extract basin ID and geometry info\n",
    "basins = basins.rename(columns={'HYBAS_ID': 'HB_L6'}) # rename basin ID to align with remaining workflow (working at hydrobasin level 6)\n",
    "\n",
    "records = [] # empty list to save results to\n",
    "# Loop over all rasters\n",
    "for tif_path in tqdm(sorted(RASTER_DIR.glob(\"*.tif\")), desc=\"Running Zonal Stats\"):\n",
    "    stem = tif_path.stem  # e.g. \"cwatm_gfdl-esm4_ssp126_2030_rp010\"\n",
    "    parts = stem.split(\"_\")\n",
    "    if len(parts) != 5:\n",
    "        print(f\"Skipping unexpected filename: {stem}\")\n",
    "        continue\n",
    "\n",
    "    hydro, clim, scen, epoch, rp_str = parts\n",
    "    rp = int(rp_str.replace(\"rp\", \"\"))\n",
    "\n",
    "    # Zonal stats: mean per basin\n",
    "    zs = zonal_stats(\n",
    "        basins,\n",
    "        tif_path,\n",
    "        stats=[\"mean\"],\n",
    "        nodata=-9999,\n",
    "        all_touched=True   # calculate stats on any cells that touch region\n",
    "    )\n",
    "    means = [z[\"mean\"] for z in zs]\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        'HB_L6': basins['HB_L6'].values,\n",
    "        'hydro': hydro,\n",
    "        'climate': clim,\n",
    "        'climate_scenario': scen,\n",
    "        'period': int(epoch),\n",
    "        'return_period': rp,\n",
    "        \"basin_mean_value\": means,\n",
    "    })\n",
    "\n",
    "    records.append(tmp)\n",
    "\n",
    "# Combine everything\n",
    "future_rp_shifts = pd.concat(records, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08507358-e427-4144-a260-8791c97be7bd",
   "metadata": {},
   "source": [
    "##### 9. Create hydrological basin return period change dataframe and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "551c8cd2-85a6-4a19-9fe6-023d1db3cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark.DESKTOP-UFHIN6T\\AppData\\Local\\Temp\\ipykernel_19632\\3398958568.py:2: FutureWarning: The provided callable <function mean at 0x00000177B48CE820> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  grouped = (future_rp_shifts.groupby(['HB_L6', 'hydro', 'climate_scenario', 'period', 'return_period'])\n"
     ]
    }
   ],
   "source": [
    "# Calculate stats across climate models for each hydro model and scenario:epoch:rp combination\n",
    "grouped = (future_rp_shifts.groupby(['HB_L6', 'hydro', 'climate_scenario', 'period', 'return_period'])\n",
    "    .agg(q95=(\"basin_mean_value\", lambda x: x.quantile(0.05)),\n",
    "        q50=(\"basin_mean_value\", lambda x: x.quantile(0.50)),\n",
    "        q05=(\"basin_mean_value\", lambda x: x.quantile(0.95)),\n",
    "        mean=(\"basin_mean_value\", np.mean)))\n",
    "grouped.columns.name = \"stat\"\n",
    "# Convert to stacked dataframe for saving\n",
    "long_stats = (\n",
    "    grouped\n",
    "    .stack()\n",
    "    .reset_index(name=\"new_rp_value\")\n",
    ")\n",
    "# Save to CSV\n",
    "long_stats.to_csv(FUTURE_BASINS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sovereign-risk",
   "language": "python",
   "name": "sovereign-risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
