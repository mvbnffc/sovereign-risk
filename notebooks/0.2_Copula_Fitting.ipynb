{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import live code changes in\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, chi2\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.univariate import UniformUnivariate\n",
    "\n",
    "from sovereign.flood import combine_glofas, extract_discharge_timeseries, fit_gumbel_distribution, calculate_uniform_marginals\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set filepaths and provide data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filepaths\n",
    "root = Path.cwd().parent # find project root\n",
    "glofas_path = os.path.join(root, 'inputs', 'flood', 'dependence', 'glofas')\n",
    "basin_outlets_path = os.path.join(root, 'inputs', 'flood', 'dependence', 'basin_outlets_match.csv') # Lat Lon points at basin outlets\n",
    "copula_samples_path = os.path.join(root, 'outputs', 'flood', 'dependence', 'copulas', 'copula_random_numbers.gzip')\n",
    "# Dependence analysis parameters\n",
    "start_year = 1979 # start year for discharge data\n",
    "end_year = 2016 # end year for discharge data\n",
    "area_filter = 500 # not considering rivers with upstream areas below 500 km^2\n",
    "n_samples = 10000 # number of copula samples to fit (will be used in subsequent Monte Carlo simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Discharge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load GloFAS river discharge data and upstream accumulating area data\n",
    "glofas_data = combine_glofas(start_year, end_year, glofas_path, area_filter)\n",
    "\n",
    "# Step 2: Load the basin outlet file, perform some data checks (to ensure we have valid discharge timeseries at each basin outlet point), and then extract discharge timeseries for each basin\n",
    "basin_outlets = pd.read_csv(basin_outlets_path)\n",
    "# Note to align the two datasets we need to make the following adjustment to lat lons (based on previous trial and error)\n",
    "basin_outlets['Latitude'] = basin_outlets['Latitude'] + 0.05/2\n",
    "basin_outlets['Longitude'] = basin_outlets['Longitude'] - 0.05/2\n",
    "# Extract discharge timeseries\n",
    "basin_timeseries = extract_discharge_timeseries(basin_outlets, glofas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Gumbel Distribution and Compute Uniform Marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gumbel distribution using annual maxima\n",
    "gumbel_params, fit_quality = fit_gumbel_distribution(basin_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute uniform marginals\n",
    "uniform_marginals = calculate_uniform_marginals(basin_timeseries, gumbel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each basin to their L3 (major) river basin (assuming independence across major river basins). 5 total L3 basins\n",
    "marginals = pd.DataFrame(uniform_marginals)\n",
    "l3_basins = basin_outlets.HYBAS_ID_L3.unique()\n",
    "l3_data = {}\n",
    "\n",
    "for basin in l3_basins:\n",
    "    associated_l6_basins = list(basin_outlets[basin_outlets.HYBAS_ID_L3 == basin].HYBAS_ID_L6.unique())\n",
    "    data = marginals[associated_l6_basins]\n",
    "    l3_data[basin] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-Copula Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_samples = {}\n",
    "\n",
    "for basin, data in l3_data.items():\n",
    "    corr_matrix = data.corr().values\n",
    "    mu = np.zeros(len(corr_matrix))\n",
    "    s = chi2.rvs(df=3, size=n_samples)[:, np.newaxis]\n",
    "    Z = np.random.multivariate_normal(mu, corr_matrix, n_samples)\n",
    "    X = np.sqrt(3/s)*Z\n",
    "    U = t.cdf(X, df=3)\n",
    "    t_samples[basin] = pd.DataFrame(U, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = pd.DataFrame()\n",
    "for basin, sample in t_samples.items():\n",
    "    generated_samples = pd.concat([generated_samples, sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples.to_parquet(copula_samples_path, compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
